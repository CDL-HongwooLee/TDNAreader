import subprocess as sp
import pandas as pd
import os
import re
from pathlib import Path
from typing import Optional, List, Tuple, Dict
from multiprocessing import Pool
import numpy as np
import argparse
from utils import run_cmd
import sys
def parse_args():
    parser = argparse.ArgumentParser(description = 
    '''
    T-DNAreader is a bioinformatics tool designed to identify T-DNA insertion sites (TISs) in plant genomes using NGS data.
    Contact address: miso5103@snu.ac.kr
    ''', formatter_class = argparse.RawTextHelpFormatter)
    parser.add_argument('-bam', dest = 'bam', required=True, default = None, help = 
    '''Input BAM or SAM files.
Separate multiple files with commas (e.g. rep1.bam,rep2.bam).
For paired-end, separate pairs with colons (e.g. rep1_1.bam:rep1_2.bam).
    ''')
    parser.add_argument('-ctl', dest = 'control', default = None, help = 
    '''(Optional) Control (WT) BAM files.
For paired-end, separate pairs with colons (e.g. rep1_1.bam:rep1_2.bam).
    ''')
    parser.add_argument('-tis', dest = 'tis', required=True, help = 
    '''{prefix}.TDNA.txt file generated by TDNAreader.py
    ''')
    parser.add_argument('-read', dest = 'read', required=True, help = 
    '''{prefix}.read_info.txt file generated by TDNAreader.py
    ''')
    parser.add_argument('-o', dest = 'outdir', default = Path('./'), help = 
    '''Output directory.
Default = './'
    ''')
    parser.add_argument('-p', dest = 'prefix', required=True, help = 
    '''Prefix for naming output files.
    ''')
                        
    parser.add_argument('-bed', dest='bed', default = None, help = 
    '''(Optional) BED6 file with genomic features. 
Column 4 should contain gene IDs.
Overlapping TISs will be annotated.
    ''')
    parser.add_argument('-gtf', dest='gtf', default = None, help = 
    '''(Optional) GTF file for STAR alignment and gene annotation.
    ''')
    parser.add_argument('-t', dest = 'thread', default = 1, type = int, help = 
    '''Number of threads to use. 
Default = 1.
    ''')
    parser.add_argument('--paired', dest = 'paired', action='store_true', help = 
    '''Inputs are from paired-end data.
Separate pairs with ':'.
    ''')
    return parser.parse_args()

def parse_inputs(bam: str, control: str, prefix: str, paired: bool):
    transgenic = bam.split(',')
    controls = control.split(',') if control else []
    
    bam_files = []
    for inp in transgenic:
        bam_files.extend(inp.split(':') if paired else [inp])
    for inp in controls:
        bam_files.extend(inp.split(':') if paired else [inp])

    transgenic_prefixes = [] ; control_prefixes = []
    for idx, inp in enumerate(transgenic, start=1):
        if paired and ':' in inp:
            transgenic_prefixes.extend([f"{prefix}_{idx}-F", f"{prefix}_{idx}-R"])
        else:
            transgenic_prefixes.append(f"{prefix}_{idx}")
    for idx, inp in enumerate(controls, start=1):
        if paired and ':' in inp:
            control_prefixes.extend([f"{prefix}_control_{idx}-F", f"{prefix}_control_{idx}-R"])
        else:
            control_prefixes.append(f"{prefix}_control_{idx}")

    return transgenic_prefixes + control_prefixes, bam_files

def make_track_dir(out_dir: Path, prefix: str):
    track_dir = out_dir / f"{prefix}_track"
    track_dir.mkdir(exist_ok=True)
    return track_dir

def adjust_coordinates(row: pd.Series):
    first_entry = row['TISs'].split(',')[0]
    parts = first_entry.split('::')
    orientation = parts[2]
    if orientation == 'R':
        row['start'] = row['end'] - 1
    elif orientation == 'L':
        row['end'] = row['start'] + 1
    if len(row['TISs'].split(',')) > 1:
        last_part = row['TISs'].split(',')[-1].split('::')[1]
        row['TISs'] = f'{parts[0]}::{parts[1]}-{last_part}::{parts[2]}::{parts[3]}'
    return row

def rangeBed(out_dir: Path, tis: Path, prefix: str, bed: Path, range1=500, range2=2000, range3=1000):
    track_dir = make_track_dir(out_dir, prefix)
    tis_df = pd.read_csv(tis, sep='\t', header=0)

    tis_bed = tis_df[['chrom', 'start', 'end', 'TISs', 'gene']].copy()
    tis_bed['score'] = 0
    tis_bed['strand'] = tis_df['orientation']

    tis_visualize = tis_bed[['chrom', 'start', 'end', 'TISs', 'score', 'strand']].copy()
    tis_visualize = tis_visualize.apply(adjust_coordinates, axis=1)
    tis_visualize_bed = track_dir / f"{prefix}.TDNA.bed"
    tis_visualize.to_csv(tis_visualize_bed, sep='\t', index=False, header=False)
    tis_bed['TIS_range'] = tis_visualize['TISs']
    tis_dic = dict(zip(tis_bed['TIS_range'], tis_bed['TISs']))
    
    gene_list = set(g for g in tis_df['gene'].values if ',' not in g) - set('.')
    if bed:
        if gene_list:
            bed_df = pd.read_csv(bed, sep='\t', header=0, names=['chrom', 'start', 'end', 'gene', 'score', 'strand']).query('gene in @gene_list')
            bed_df['start'] -= range1
            bed_df['end'] += range1
            genomic_regions = bed_df[['chrom', 'start', 'end', 'gene']].copy()
        else:
            genomic_regions = pd.DataFrame(columns=['chrom', 'start', 'end', 'gene'])

        gene_overlapping_list = set(g for g in tis_df['gene'].values if ',' in g)
        for overlapping_genes in gene_overlapping_list:
            genes = overlapping_genes.split(',')
            overlapping_df = pd.read_csv(bed, sep='\t', header=0, names=['chrom', 'start', 'end', 'gene', 'score', 'strand']).query('gene in @genes')
            overlapping_region = pd.DataFrame({'chrom': [overlapping_df['chrom'].values[0]], 'start': [np.min(overlapping_df['start'].values)], 'end': [np.max(overlapping_df['end'].values)], 'gene': [overlapping_genes]})
            genomic_regions = pd.concat([genomic_regions, overlapping_region], ignore_index=True)#.query('end - start <= 100000')
        
        mask_small = (genomic_regions['end'] - genomic_regions['start']) <= 100000
        small_df = genomic_regions.loc[mask_small, ['chrom', 'start', 'end', 'gene']]
        too_big_genes = set(genomic_regions.loc[~mask_small, 'gene'])
        if too_big_genes:
            alternative_regions = tis_bed.query('gene in @too_big_genes')[['chrom', 'start', 'end', 'gene']]
            alternative_regions['start'] -= range2
            alternative_regions['end'] += range2
            genomic_regions = pd.concat([small_df[['chrom', 'start', 'end']], alternative_regions[['chrom', 'start', 'end']]], ignore_index=True)
        else:
            genomic_regions = genomic_regions.iloc[:,:3]
    else:
        genomic_regions = pd.DataFrame(columns=['chrom', 'start', 'end'])
    
    inter_tmp = track_dir / f"{prefix}.TDNA.intergenic.tmp"
    inter_bed = track_dir / f"{prefix}.TDNA.intergenic.bed"
    cmd = (
        f"tail -n+2 {tis} | awk '$5 == \".\"' | awk '{{print $1, $2-{range2}, $3+{range2}}}' OFS='\t' | sort -k1,1 -k2,2n > {inter_tmp} && "
        f"bedtools merge -i {inter_tmp} > {inter_bed}"
    )
    run_cmd(cmd, shell=True)
    intergenicdf = pd.read_csv(inter_bed, sep='\t', header=None, names=['chrom', 'start', 'end'])

    region_df = pd.concat([genomic_regions, intergenicdf], ignore_index=True).sort_values(['chrom', 'start', 'end'])
    region_bed = track_dir / f"{prefix}.region.bed"
    region_df.to_csv(region_bed, sep='\t', index=False, header=False)
    region_list = list((region_df['chrom'] + ':' + region_df['start'].apply(str) + ':' + region_df['end'].apply(str)).values)
    return track_dir, region_list, tis_dic, tis_visualize_bed

def cigar_to_block(cigar: str, start: int, end: int, clip_pos: str):

    pattern = re.compile(r'(\d+)([MIDNSHP=X])')
    tokens = pattern.findall(cigar)
    
    left_soft = 0
    right_soft = 0
    
    if clip_pos == 'L' and tokens and tokens[0][1] == 'S':
        left_soft = int(tokens[0][0])
        new_start = start - left_soft
        thickStart = start
    else:
        new_start = start
        thickStart = start
    
    if clip_pos == 'R' and tokens and tokens[-1][1] == 'S':
        right_soft = int(tokens[-1][0])
        new_end = end + right_soft
        thickEnd = end
    else:
        new_end = end
        thickEnd = end

    block_sizes = []
    block_starts = []
    ref_pos = 0
    current_block_size = 0
    current_block_start = None
    
    first_block_incorporated = False

    for length_str, op in tokens:
        length = int(length_str)
        if op in ('M', '=', 'X', 'D'):
            if current_block_start is None:
                if (clip_pos == 'L') and (not first_block_incorporated) and (left_soft > 0):
                    current_block_start = 0
                    first_block_incorporated = True
                else:
                    current_block_start = ref_pos
            current_block_size += length
            ref_pos += length
        elif op == 'N':
            if current_block_size > 0 and current_block_start is not None:
                block_sizes.append(current_block_size)
                block_starts.append(current_block_start)
                current_block_size = 0
                current_block_start = None
            ref_pos += length
        elif op in ('I', 'H', 'P'):
            pass
    
    if current_block_size > 0 and current_block_start is not None:
        if clip_pos == 'R' and right_soft > 0:
            current_block_size += right_soft
        block_sizes.append(current_block_size)
        block_starts.append(current_block_start)
    
    if block_starts and block_sizes:
        computed_new_end = new_start + (block_starts[-1] + block_sizes[-1])
        new_end = computed_new_end
    
    block_n = len(block_sizes)
    blockSizes_str = ",".join(str(b) for b in block_sizes) + ","
    blockStarts_str = ",".join(str(b) for b in block_starts) + ","
    
    return new_start, new_end, thickStart, thickEnd, block_n, blockSizes_str, blockStarts_str

def make_tdna_bam_bed(track_dir: Path, prefix: str, tis_dic: Dict[str, str], read_info_df: pd.DataFrame):
    
    read_info_df = read_info_df.query('clip_pos != "D"').reset_index(drop=True)
    read_info_df['end'] += 1
    out_df = read_info_df[['chrom', 'start', 'end', 'readid', 'TIS']].copy()
    out_df.loc[:,'score'] = 0
    out_df['strand'] = '.'

    out_df['start2'] = out_df['start']
    out_df['end2'] = out_df['end']
    out_df.loc[:,'color'] = 0 
    out_df[['start', 'end', 'start2', 'end2', 'blockN', 'blockSizes', 'blockStarts']] = read_info_df.apply(
        lambda row: pd.Series(cigar_to_block(row['cigar'], row['start'], row['end'], row['clip_pos'])),
        axis=1
    )
    
    for tis_range, tis in tis_dic.items():   
        s = tis_range.split('::')
        name = f'{s[0]}_{s[1]}_{s[2]}'
        if s[2] == "R":
            border = tis.split(',')[-1]
            region_df = out_df.query('TIS == @border').sort_values('end', ascending=[False]).drop(columns='TIS')
        elif s[2] == "L":
            border = tis.split(',')[0]
            region_df = out_df.query('TIS == @border').sort_values('start', ascending=[True]).drop(columns='TIS')
        region_df.to_csv(f'{track_dir}/{prefix}.{name}.junction_reads.bed', sep='\t', index=False, header=False)
    
def bam_to_bigwig(track_dir: Path, region: str, bam: Path, p: str):
    s = region.split(':')
    re = f"{s[0]}_{s[1]}-{s[2]}"
    cmd = (
        f"bamCoverage -b {bam} -o {track_dir}/{p}.{re}.RPKM.bw --outFileFormat bigwig --binSize 1 --normalizeUsing RPKM --region {region}"
    )
    with open(os.devnull, 'w') as devnull:
        run_cmd(cmd, shell=True, silence=devnull)

def bam_to_bigwig_parallel(track_dir: Path, region_list: List[str], bam_files: List[Path], prefixes: str, t: int):    
    with Pool(t) as pool:
        pool.starmap(
            bam_to_bigwig,
            [
                (track_dir, region, bam, prefix)
                for region in region_list
                for bam, prefix in zip(bam_files, prefixes)
            ]
        )

def bigwig_to_bedgraph(track_dir: Path, prefix: str, name: str, paired: bool):
    if paired:
        cmd = (
            f"bigWigMerge {track_dir}/{prefix}-F.{name}.RPKM.bw {track_dir}/{prefix}-R.{name}.RPKM.bw {track_dir}/{prefix}.{name}.RPKM.bedGraph"
        )
    else:
        cmd = (
            f"bigWigToBedGraph {track_dir}/{prefix}.{name}.RPKM.bw {track_dir}/{prefix}.{name}.RPKM.bedGraph"
        )
    with open(os.devnull, 'w') as devnull:
        run_cmd(cmd, shell=True, silence=devnull)

def bigwig_to_bedgraph_parallel(region_list: List[str], track_dir: Path, prefixes: List[str], paired: bool, t: int):

    name_list = [f"{r.split(':')[0]}_{r.split(':')[1]}-{r.split(':')[2]}" for r in region_list]
    with Pool(t) as pool:
        pool.starmap(
            bigwig_to_bedgraph,
            [
                (track_dir, prefix, name, paired)
                for prefix in prefixes
                for name in name_list
            ]
        )

    for p in prefixes:
        bedgraphs = ' '.join([f'{track_dir}/{p}.{re}.RPKM.bedGraph' for re in name_list])
        cmd = (
            f"cat {bedgraphs} | sort -k1,1 -k2,2n -k3,3n > {track_dir}/{p}.merged.RPKM.bedGraph"
        )
        run_cmd(cmd, shell=True)

def create_track_backbone(track_file: Path, tis: Path, gtf: Optional[Path], bed: Optional[Path]):
    with open(track_file, 'w') as o:
            ## Annotation track
        o.write('''
[x-axis]
[spacer]
height = 0.5
                ''')
        if gtf:
            o.write(f'''
[GTF]
file = {gtf}
title = genes
height = 1
color = #00afb9
fontsize = 10
file_type = gtf
gene_rows = 3
                    ''')
        elif bed:
            o.write(f'''
[BED]
file = {bed}
title = genes
height = 1
color = #00afb9
fontsize = 10
file_type = bed
gene_rows = 3
                    ''')
        o.write('''
[spacer]
height = 0.5
                ''')
        
                ## T-DNA sites
        o.write(f'''
[TDNA]
file = {tis}
title = TDNA
height = 1.5
color = #e63946
fontsize = 10
file_type = bed
style = flybase
arrowhead_fraction = 0.01
gene_rows = 3
                ''')

def add_bedgraph_tack(track_file: Path, bedgraph: Path, name: str, color: str):
    with open(track_file, 'a') as o:
        o.write(f'''
[bedGraph]
file = {bedgraph}
title = {name}
height = 2
color = {color}
min_value = 0
file_type = bedgraph
        ''')

def add_read_info_track(track_file: Path, junc_reads: Path, color: str, color_utr='#e63946'):
    with open(track_file, 'a') as o:
        o.write(f'''
[Reads]
file = {junc_reads}
title = reads
height = 10
color = {color}
color_utr = {color_utr}
fontsize = 7
gene_rows = 12
        ''')

def create_track(track_dir: Path, prefix: str, prefixes: List[str], tis_dic: Dict[str,str], tdna: Path, gtf: Optional[Path], bed: Optional[Path], color1='#ffac81', color2='#3a6ea5', color3='#3a6ea5'):

    coverage_track = track_dir / f'{prefix}.track.ini'
    create_track_backbone(coverage_track, tdna, gtf, bed)
    for pfx in prefixes:
        
        bedgraph = Path(track_dir / f'{pfx}.merged.RPKM.bedGraph').absolute()
        if 'control' in pfx:
            add_bedgraph_tack(coverage_track, bedgraph, pfx, color1)
        else:
            add_bedgraph_tack(coverage_track, bedgraph, pfx, color2)

    for tis_range in tis_dic.keys():
        s = tis_range.split('::')
        name = f"{s[0]}_{s[1]}_{s[2]}"
        read_track = track_dir/ f'{prefix}.{name}.reads.ini'
        junc_reads = Path((track_dir / f'{prefix}.{name}.junction_reads.bed')).absolute()
        create_track_backbone(read_track, tdna, gtf, bed)
        add_read_info_track(read_track, junc_reads, color3)

def plot_image(track_dir: Path, out_dir: Path, prefix: str, region_list: List[str], tis_dic :Dict[str,str], range3= 1000, format='pdf'):
    with open(os.devnull, 'w') as devnull:
        for region in region_list:
            s = region.split(':')
            name = f'{s[0]}_{s[1]}-{s[2]}'
            re = f'{s[0]}:{s[1]}-{s[2]}'
            cmd = (
                f"pyGenomeTracks --tracks {track_dir}/{prefix}.track.ini --fontSize 10 --region {re} --trackLabelFraction 0.15 --width 60 "
                 f"--outFileName {out_dir}/{prefix}.{name}.track.{format}"
            )
            run_cmd(cmd, shell=True, silence=devnull)
        for tis_range in tis_dic.keys():
            s = tis_range.split('::')
            if '-' in s[1]:
                if s[2] == "R":
                    re = f'{s[0]}:{int(s[1].split("-")[1])-range3}-{int(s[1].split("-")[1])+range3}'
                else:
                    re = f'{s[0]}:{int(s[1].split("-")[0])-range3}-{int(s[1].split("-")[0])+range3}'
            else:
                re = f'{s[0]}:{int(s[1])-range3}-{int(s[1])+range3}'
            name = f"{s[0]}_{s[1]}_{s[2]}"
            cmd = (
                f"pyGenomeTracks --tracks {track_dir}/{prefix}.{name}.reads.ini --fontSize 10 --region {re} --trackLabelFraction 0.15 --width 60 "
                f"--outFileName {out_dir}/{prefix}.{name}.reads.{format}"
            )
            run_cmd(cmd, shell=True, silence=devnull)

def main():
    args = parse_args()
    out_dir = Path(args.outdir)
    out_dir.mkdir(parents=True, exist_ok=True)
    tis = Path(args.tis)
    read_info_df = pd.read_csv(args.read, sep='\t', header=0)
    track_dir, region_list, tis_dict, tis_bed = rangeBed(out_dir, tis, args.prefix, args.bed)
    make_tdna_bam_bed(track_dir, args.prefix, tis_dict, read_info_df)

    prefixes, bam_files = parse_inputs(args.bam, args.control, args.prefix, args.paired)
    bam_to_bigwig_parallel(track_dir, region_list, bam_files, prefixes, args.thread)
    if args.paired:
        prefixes = [p[:-2] for p in prefixes[::2]]
    bigwig_to_bedgraph_parallel(region_list, track_dir, prefixes, args.paired, args.thread)
    
    gtf = Path(args.gtf).absolute() if args.gtf else None
    bed = Path(args.bed).absolute() if args.bed else None
    create_track(track_dir, args.prefix, prefixes, tis_dict, Path(tis_bed).absolute(), gtf, bed)
    plot_image(track_dir, out_dir, args.prefix, region_list, tis_dict)

if __name__ == '__main__':
    main()